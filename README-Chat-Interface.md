# AI Chat Interface mit Ollama

Eine moderne, responsive Chat-Interface-Anwendung mit direkter Anbindung an Ollama und erweiterten System-Prompt-Funktionen.

## ğŸš€ Features

- **Moderne UI**: Responsives Design mit ansprechendem Gradient-Hintergrund
- **Ollama Integration**: Direkte Verbindung zu lokalen Ollama-Modellen
- **System Prompts**: Anpassbare System-Prompts mit vorgefertigten Presets
- **Modell-Auswahl**: Dynamische Auswahl verfÃ¼gbarer Ollama-Modelle
- **Echtzeit-Chat**: Sofortige Antworten mit Typing-Indikator
- **Konversationsspeicher**: VollstÃ¤ndiger Chat-Verlauf wÃ¤hrend der Session
- **Responsive Design**: Optimiert fÃ¼r Desktop und Mobile

## ğŸ“‹ Voraussetzungen

1. **Ollama installiert**: [Ollama Download](https://ollama.ai/download)
2. **Mindestens ein Modell**: z.B. `ollama pull llama3.2`
3. **Ollama Server lÃ¤uft**: `ollama serve`

## ğŸ› ï¸ Installation & Setup

### 1. Ollama installieren
```bash
# FÃ¼r Windows: Installer von https://ollama.ai/download herunterladen
# FÃ¼r macOS: brew install ollama
# FÃ¼r Linux: curl -fsSL https://ollama.ai/install.sh | sh
```

### 2. Modell herunterladen
```bash
# Empfohlene Modelle
ollama pull llama3.2        # Schnell und effizient
ollama pull llama3.1        # GrÃ¶ÃŸer, mehr Funktionen
ollama pull mistral         # Alternativer Ansatz
ollama pull codellama       # Speziell fÃ¼r Code
ollama pull phi3           # Kompakt und schnell
```

### 3. Ollama Server starten
```bash
ollama serve
```
Der Server lÃ¤uft standardmÃ¤ÃŸig auf `http://localhost:11434`

### 4. Chat-Interface Ã¶ffnen
- Ã–ffnen Sie `chat-interface.html` in Ihrem Browser
- Die Anwendung prÃ¼ft automatisch die Verbindung zu Ollama

## ğŸ¯ Nutzung

### System Prompts
Das Interface bietet vier vordefinierte System-Prompt-Presets:

1. **ğŸ“ Standard Assistent**: Allgemeine Hilfe und UnterstÃ¼tzung
2. **ğŸ’» Code Experte**: Spezialisiert auf Programmierung und Softwareentwicklung
3. **ğŸ¨ Kreativer Schreiber**: FÃ¼r kreative Texte und Storytelling
4. **ğŸ“Š Daten Analyst**: FÃ¼r Datenanalyse und Statistiken

Sie kÃ¶nnen auch eigene System Prompts in der Textarea eingeben.

### Chat-Funktionen
- **Nachricht senden**: Enter-Taste oder Send-Button
- **Neue Zeile**: Shift + Enter
- **Chat lÃ¶schen**: "Chat lÃ¶schen" Button im Header
- **Modell wechseln**: Dropdown-MenÃ¼ in der Sidebar

### Verbindungsstatus
- ğŸŸ¢ **GrÃ¼n**: Erfolgreich mit Ollama verbunden
- ğŸ”´ **Rot**: Keine Verbindung zu Ollama

## ğŸ”§ Konfiguration

### API-Endpoint Ã¤ndern
Falls Ollama auf einem anderen Port oder Server lÃ¤uft:
```javascript
// In der Datei chat-interface.html, Zeile ~90
const OLLAMA_API_URL = 'http://localhost:11434'; // Hier Ã¤ndern
```

### Neue System-Prompt-Presets hinzufÃ¼gen
```javascript
// In der Datei chat-interface.html, Zeile ~100
const systemPrompts = {
    // Bestehende Presets...
    custom: 'Ihr eigener System-Prompt hier'
};
```

Dann im HTML den entsprechenden Button hinzufÃ¼gen:
```html
<button class="preset-btn" onclick="setSystemPrompt('custom')">ğŸ”§ Eigener Preset</button>
```

## ğŸš¨ Fehlerbehebung

### "Ollama ist nicht verfÃ¼gbar"
1. PrÃ¼fen Sie, ob Ollama installiert ist: `ollama --version`
2. Starten Sie den Ollama-Server: `ollama serve`
3. Testen Sie die API: `curl http://localhost:11434/api/tags`

### "Keine Modelle gefunden"
1. Laden Sie ein Modell herunter: `ollama pull llama3.2`
2. PrÃ¼fen Sie verfÃ¼gbare Modelle: `ollama list`

### CORS-Fehler
Ollama sollte standardmÃ¤ÃŸig CORS fÃ¼r localhost erlauben. Falls Probleme auftreten:
```bash
# Ollama mit CORS-Einstellungen starten
OLLAMA_ORIGINS="*" ollama serve
```

### Browser-KompatibilitÃ¤t
- Chrome/Edge: âœ… VollstÃ¤ndig unterstÃ¼tzt
- Firefox: âœ… VollstÃ¤ndig unterstÃ¼tzt
- Safari: âœ… VollstÃ¤ndig unterstÃ¼tzt

## ğŸ“± Mobile Nutzung

Das Interface ist vollstÃ¤ndig responsiv und optimiert fÃ¼r:
- Smartphones (ab 320px Breite)
- Tablets (ab 768px Breite)
- Desktop (ab 1024px Breite)

Auf mobilen GerÃ¤ten wird die Sidebar Ã¼ber dem Chat-Bereich angezeigt.

## ğŸ”’ Sicherheit & Datenschutz

- **Lokale Verarbeitung**: Alle Daten bleiben auf Ihrem Computer
- **Keine Cloud-Verbindung**: Direkter Zugriff auf lokale Ollama-Installation
- **Session-basiert**: Chat-Verlauf wird nicht permanent gespeichert

## ğŸ¤ Mitwirken

VerbesserungsvorschlÃ¤ge und Fehlermeldungen sind willkommen! Das Interface kann leicht erweitert werden um:
- Weitere System-Prompt-Presets
- Chat-Export-Funktionen
- Erweiterte Modell-Konfigurationen
- Themes und Anpassungen

## ğŸ“„ Lizenz

Dieses Projekt ist Open Source und kann frei verwendet und modifiziert werden.
